{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Sampling pour des réseaux de neurones peu profond (SNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique ici la théorie de l'optimal sampling dans le cadre des réseaux de neurones peu profond. \n",
    "\n",
    "\n",
    "On définit un réseau de neurone peu profond typique $\\Phi_\\theta : X = \\mathbb{R}^n \\rightarrow \\mathbb{R} $ de taille $m \\in \\mathbb{N}$ et de paramètre $\\theta = (A_1, b_1, A_0, b_0) \\in \\mathbb{R}^{1 \\times m} \\times \\mathbb{R}^{m \\times n}\\times \\mathbb{R}^m$ par :\n",
    "\\begin{equation}\n",
    "    \\Phi_{(A_1, b_1, A_0, b_0)}(x) = A_1\\sigma(A_0x + b_0) + b_1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des fonctions \n",
    "\n",
    "Dans un premier temps on définit toutes les fonctions utiles à l'application de l'optimal sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliotheques\n",
    "import numpy as np\n",
    "from scipy.special import legendre\n",
    "from numpy.polynomial import Polynomial\n",
    "from numpy.polynomial.legendre import Legendre\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import cond\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import uniform\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "from numpy.linalg import lstsq\n",
    "import numpy.random as rnd\n",
    "import torch\n",
    "from scipy.linalg import sqrtm\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.func import grad\n",
    "from numpy.polynomial.hermite import hermgauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du réseau de neurones simple\n",
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        #self.A0 = torch.nn.Parameter(torch.randn(hidden_dim, input_dim))  # Poids\n",
    "        #self.b0 = torch.nn.Parameter(torch.randn(hidden_dim))  # Biais\n",
    "        self.activation = torch.sigmoid  # Fonction d'activation σ\n",
    "        #self.A1 = torch.nn.Parameter(torch.randn(1, hidden_dim))  # Poids 1\n",
    "        #self.b1 = torch.nn.Parameter(torch.randn(1))  # Biais 1\n",
    "\n",
    "    #def forward(self, x):\n",
    "    #    return torch.matmul( self.A1, self.activation(torch.matmul(self.A0, x) + self.b0) ) + self.b1\n",
    "\n",
    "    def forward(self, x, A0, b0, A1, b1):\n",
    "        return torch.matmul(A1, self.activation(torch.matmul(A0, x) + b0) ) + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_ij(n, m, i, j):\n",
    "\n",
    "    # INPUT : 2 entiers n et m qui représentent la taill de la matrice, et 2 entiers i et j, qui indiquent respectivement une ligne et une colonne\n",
    "    # OUTPUT : vecteur de base des matrice de taille n x m avec des 0 partout sauf pour le coefficient i, j\n",
    "\n",
    "    E = np.zeros((n, m))\n",
    "    E[i, j] = 1\n",
    "    return torch.tensor(E)\n",
    "\n",
    "\n",
    "def e_j(n, j):\n",
    "\n",
    "    # INPUT : 1 entier n qui représente la taill du vecteur, et 1 entier i, qui indique une ligne\n",
    "    # OUTPUT : vecteur de base des vecteurs de taille n avec des 0 partout sauf pour le coefficient i\n",
    "\n",
    "    e = np.zeros(n)  \n",
    "    e[j] = 1 \n",
    "    return torch.tensor(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(2.0760, dtype=torch.float64), tensor(0.2393, dtype=torch.float64), tensor(0.4542, dtype=torch.float64), tensor(0.4201, dtype=torch.float64), tensor(2.0422, dtype=torch.float64), tensor(0.2058, dtype=torch.float64), tensor(0.4209, dtype=torch.float64), tensor(0.3863, dtype=torch.float64), tensor(3.0116, dtype=torch.float64), tensor(1.1749, dtype=torch.float64), tensor(1.3898, dtype=torch.float64), tensor(1.3557, dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "def calcul_grad_SNN(A0, b0, A1, b1, h):\n",
    "\n",
    "    # INPUT : un vecteur x, 4 matrices A0, b0, A1, b1. Ces vecteurs ont été utlisé dans le réseau qui a pour sortie A1*sigma(A0x + b0) + b1\n",
    "    # OUTPUT : la linéarisation du réseau évalué en x\n",
    "\n",
    "    d = hidden_dim * (input_dim + 2) + 1\n",
    "    liste_grad = []\n",
    "    for j in range(hidden_dim):\n",
    "        liste_grad.append(lambda x, j = j: (torch.sigmoid(torch.matmul(A0, x)) + b0)[j])\n",
    "    for i in range(input_dim):\n",
    "        for j in range(hidden_dim):\n",
    "            Eij = E_ij(input_dim, hidden_dim, i, j)\n",
    "            liste_grad.append(lambda x, j = j: (torch.sigmoid(torch.matmul(A0 + torch.matmul(Eij, h), x)) + b0)[j])\n",
    "    for j in range(hidden_dim):\n",
    "        ej = e_j(hidden_dim, j)\n",
    "        liste_grad.append(lambda x, j = j: (torch.sigmoid(torch.matmul(A0, x)) + b0 + torch.matmul(ej, h))[j])\n",
    "\n",
    "    return liste_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_hermite_integration(phi_i, phi_j, A0, b0, n_quadra):\n",
    "    \n",
    "    # INPUT : 2 fonctions phi_i, phi_j, et les matrices A0 et b0 du réseau. n_quadra est le nombre de points de quadrature\n",
    "    # OUTPUT : le produit scalaire ( phi_i, phi_j ) par quadrature de gauss\n",
    "\n",
    "    nodes, weights = hermgauss(n_quadra)  # Récupération des points et poids\n",
    "    integral = np.sum(weights * phi_i(A0 * nodes + b0) * phi_i(A0 * nodes + b0)) / np.sqrt(np.pi)\n",
    "    return integral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_gram_SNN(A0, b0, A1, b1):\n",
    "\n",
    "    # INPUT : les paramètres du réseau A0, b0, A1, b1\n",
    "    # OUTPUT : la matrice de Gram de la base de l'espace de linéarisation\n",
    "\n",
    "    # on récupère la base associé au réseau\n",
    "    fam_gen = calcul_grad_SNN(A0, b0, A1, b1, h)\n",
    "    d = hidden_dim*(input_dim + 2) + 1\n",
    "    gram = torch.zeros(d, d)\n",
    "\n",
    "    # on itère sur chaque élément de la base\n",
    "    for i in range(d):\n",
    "        phi_i = fam_gen[i]\n",
    "        for j in range(d):\n",
    "            phi_j = fam_gen[j]\n",
    "            # on calcul le produit scalaire avec la quadrature de gauss :\n",
    "            gram[i][j] = gauss_hermite_integration(phi_i, phi_j, A0, b0, n_quadra = 20)\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_base_ON(A0, b0, A1, b1):\n",
    "\n",
    "    # INTPUT : les paramètres du neurones A0, b0, A1, b1\n",
    "    # OUTPUT : une base orthonormée de la linéarisation obtenue à partir de la famille génératrice de l'espace et de la matrice de Gram\n",
    "\n",
    "    # on calcule la famille génératrice et la matrie de gram associé à la linéarisation\n",
    "    fam_gen = calcul_grad_SNN(A0, b0, A1, b1, h)\n",
    "    gram = calcul_gram_SNN(A0, b0, A1, b1)\n",
    "\n",
    "    # calcul de la pseudo inverse\n",
    "    G_inv = torch.linalg.pinv(gram)\n",
    "    # calcul de la racine de cette pseudo inverse\n",
    "    G_inv_sqrt = sqrtm(G_inv)\n",
    "    d_min = torch.linalg.matrix_rank(G_inv)\n",
    "    d = len(fam_gen)\n",
    "\n",
    "    base_ON = torch.zeros(d_min)\n",
    "    # calcul de la base orthonormée\n",
    "    for d in range(d_min):\n",
    "        base_ON[d] = sum(G_inv_sqrt[d][j]*fam_gen[j] for j in range(d))\n",
    "\n",
    "    return base_ON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application globale de la théorie de l'optimal sampling pour les SNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque :** Contrairement à $\\texttt{optimal\\_sampling\\_approx\\_lin}$, on ne fait pas la descente de gradient directement sur $u_t$ mais sur les paramètres $\\theta_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application globale de la théorie de l'optimal sampling\n",
    "\n",
    "\n",
    "# paramètres\n",
    "\n",
    "# initialisation de theta_t pour l'optimal sampling\n",
    "A0 = torch.randn(hidden_dim, input_dim).double()\n",
    "b0 = torch.randn(hidden_dim).double()\n",
    "A1 = torch.randn(1, hidden_dim).double()\n",
    "b1 = torch.randn(1).double()\n",
    "theta_t = torch.tensor([A0, b0, A1, b1])\n",
    "\n",
    "\n",
    "n_vect = 100    # nombre de point à sampler\n",
    "m_base = 4      # taille de la base V_m\n",
    "input_dim = 1   # dimension d'entrée du SNN \n",
    "hidden_dim = 4  # dimension des paramètres\n",
    "step_size = 1/9 # step size \n",
    "liste_error = []\n",
    "liste_error_unif = []\n",
    "L_min = 3.6*10**(-4)\n",
    "nb_iter_ut = 1000\n",
    "# liste de step size pour le faire varier durant la descente de gradient\n",
    "step_size_var = [2/9*(1/(t+1))**0.9 for t in range(nb_iter_ut)] \n",
    "for i in range(10):\n",
    "    step_size_var[i] = 1\n",
    "\n",
    "\n",
    "proj = 'quasi_proj' # type de projection utilisée \n",
    "\n",
    "\n",
    "for t in range(nb_iter_ut):\n",
    "    # calcul de la projection souhaité\n",
    "    base_t = calcul_base_ON(A0, b0, A1, b1)\n",
    "    projection = calcul_proj_os(n_vect, m_base, a, b, theta_t, proj, 1/4)\n",
    "\n",
    "    # Etude AVEC optimal sampling\n",
    "    # descente de gradient\n",
    "    theta_t = theta_t - step_size*projection\n",
    "    x_os = sequential_conditional_sampling_discretisation(n_vect, m_base, base_t, a, b)\n",
    "    L_estim = L(theta_t(x_os), fonction_recherchee(x_os))\n",
    "    liste_error.append(np.abs(L_estim - L_min))\n",
    "\n",
    "\n",
    "plt.loglog([t for t in range(nb_iter_ut)], liste_error, label = 'L($theta_t$) - $L_{min,M}$ optim')\n",
    "plt.loglog([t for t in range(nb_iter_ut)], [L_min for t in range(nb_iter_ut)], '--', label = '$L_{min,M}$', color = 'red')\n",
    "plt.legend()\n",
    "plt.xlabel('t')\n",
    "plt.show()\n",
    "\n",
    "X = np.linspace(a, b, 100)\n",
    "plt.plot(X, fonction_recherchee(X), label = 'u')\n",
    "plt.plot(X, theta_t(X), label = 'approximation')\n",
    "plt.title(f'Approximation avec {proj}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
